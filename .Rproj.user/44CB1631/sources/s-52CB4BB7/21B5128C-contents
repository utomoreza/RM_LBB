---
title: "Regression Model for Predicting Wine Quality"
author: "Reza Dwi Utomo"
date: "15/02/2020"
output:
  html_document:
    highlight: zenburn
    number_sections: yes
    theme: flatly
    toc: yes
    toc_depth: 2
    toc_float:
      collapsed: yes
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction {#intro}

This article aims to accomplish [Regression Model](https://algorit.ma/course/regression-models/) course at Algoritma. The dataset used is obtained from [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Wine+Quality), i.e. Wine Quality Data Set. There are two datasets provided (red wine and white wine), but **only the red wine one will be discussed in this article.**

You could see the source code in my GitHub account [here](https://github.com/utomoreza/RM_LBB). I also wrote this article in Python language using Jupyter notebook. You could see it [here](main.ipynb).

### Dataset Background

The two datasets are related to red and white variants of the Portuguese \"Vinho Verde\" wine. For more details, consult: [Web Link](http://www.vinhoverde.pt/en/) or the reference [Cortez et al., 2009](http://dx.doi.org/10.1016/j.dss.2009.05.016). Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).

These datasets can be viewed as classification or regression tasks. The classes are ordered and not balanced (e.g. there are many more normal wines than excellent or poor ones). Outlier detection algorithms could be used to detect the few excellent or poor wines. Also, we are not sure if all input variables are relevant. So it could be interesting to test feature selection methods.

### Aim

The goal is to model red wine quality based on physicochemical tests.

### Objectives

1. To solve the final model equation

2. To output the statistical values (adjusted) R-squared, Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and MAE (Mean Absolute Error)

3. To examine the model including statistics and visualizations:

+ Assess linearity of model (parameters)
+ Assess serial independence of errors
+ Assess heteroscedasticity
+ Assess normality of residual distribution
+ Assess multicollinearity

4. To interpretate the model

5. To consider other factors, such as:

+ Are there any outliers?
+ Are there missing values?

6. To test the model using dataset test and discuss the results

### Structure

This article is arranged as follows.

1. [Introduction](#intro)
2. [Metadata](#meta)
3. [Preparation](#prep)
4. [Exploratory Data Analysis](#eda)
5. [Modelling](#model)
6. [Model Improvements](#modimprov)
7. [Results and Discussions](#resdis)
8. [Conclusions](#conc)

## Metadata {#meta}

There are 12 columns available in the dataset. They are briefly described below or you can read in [this file](winequality.names). For more information, read [Cortez et al., 2009](http://dx.doi.org/10.1016/j.dss.2009.05.016).

Input variables (based on physicochemical tests):

1. fixed acidity
2. volatile acidity
3. citric acid
4. residual sugar
5. chlorides
6. free sulfur dioxide
7. total sulfur dioxide
8. density
9. pH
10. sulphates
11. alcohol

Output variable (based on sensory data):

12. quality (score between 0 and 10)

### Source

Paulo Cortez, University of Minho, Guimarães, Portugal, http://www3.dsi.uminho.pt/pcortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis, Viticulture Commission of the Vinho Verde Region(CVRVV), Porto, Portugal @2009

### Relevant Papers

P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. *Modeling wine preferences by data mining from physicochemical properties*. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.

Available at: [Web Link](http://dx.doi.org/10.1016/j.dss.2009.05.016)

## Preparation {#prep}

Prepare the performance indicators.

```{r warning=FALSE, message=FALSE}
library(MLmetrics)
indicator <- function(model, y_pred, y_true) {
     adj.r.sq <- summary(model)$adj.r.squared
     mse <- MSE(y_pred, y_true)
     rmse <- RMSE(y_pred, y_true)
     mae <- MAE(y_pred, y_true)
     print(paste0("Adjusted R-squared: ", round(adj.r.sq, 4)))
     print(paste0("MSE: ", round(mse, 4)))
     print(paste0("RMSE: ", round(rmse, 4)))
     print(paste0("MAE: ", round(mae, 4)))
}

metrics <- function(y_pred, y_true){
     mse <- MSE(y_pred, y_true)
     rmse <- RMSE(y_pred, y_true)
     mae <- MAE(y_pred, y_true)
     print(paste0("MSE: ", round(mse, 6)))
     print(paste0("RMSE: ", round(rmse, 6)))
     print(paste0("MAE: ", round(mae, 6)))
     corPredAct <- cor(y_pred, y_true)
     print(paste0("Correlation: ", round(corPredAct, 6)))
     print(paste0("R^2 between y_pred & y_true: ", round(corPredAct^2, 6)))
}

CheckNormal <- function(model) {
     hist(model$residuals, breaks = 30)
     shaptest <- shapiro.test(model$residuals)
     print(shaptest)
     if (shaptest$p.value <= 0.05) {
          print("H0 rejected: the residuals are NOT distributed normally")
     } else {
          print("H0 failed to reject: the residuals ARE distributed normally")
     }
}

library(lmtest)
CheckHomos <- function(model){
     plot(model$fitted.values, model$residuals)
     abline(h = 0, col = "red")
     BP <- bptest(model)
     print(BP)
     if (BP$p.value <= 0.05) {
          print("H0 rejected: Error variance spreads INCONSTANTLY/generating patterns (Heteroscedasticity)")
     } else {
          print("H0 failed to reject: Error variance spreads CONSTANTLY (Homoscedasticity)")
     }
}
```

```{r}
redDF <- read.csv("winequality-red.csv", sep = ";")
redDF
```

```{r warning=FALSE, message=FALSE}
library(tidyverse)

redDF %>% is.na() %>% colSums()
```

Perfect. Since the used datasets are originally clean, we will not find any missing value. So, let's move on to explore the data.

## Exploratory Data Analysis {#eda}

In order to explore the dataset, we could use scatter plots, histograms, correlation value, and p-value.

```{r warning=FALSE, message=FALSE}
# library(psych)
# pairs.panels(redDF)

# library(ggcorrplot)
# ggcorrplot(redDF %>% cor(),
#   hc.order = TRUE, type = "lower",
#   lab = TRUE,
#   digits = 1,
#   ggtheme = ggplot2::theme_dark(),
# )

library(PerformanceAnalytics)
chart.Correlation(redDF, hist = T)
```

The preceding figure tells many things. But, in general, it shows four points: scatter plots between each variable, histograms of each variable, correlation values between each value, and p-values between each value against significance value of 0.05.

### Scatter plots {#scat}

Surprisingly, we found something interesting here. The scatter plots of between `quality` and each predictor variable form the same pattern that the target variable `quality` classifies the values into several classess, i.e. 3, 4, 5, 6, 7, and 8. To examine this case, we will go through to assess linear regression to model the data.

Moreover, there are several predictors which have strong relationship, e.g. between `fixed.acidity` and `citric.acid`. They are indicated by their tendency to have inclined or declined line. This case is discussed further in the Correlation values point below.

### Histograms {#hist}
     
Each predictor variable show value distributed appropriately. However, the target variable exhibits poor distribution. This supports the above finding from scatter plots analysis. We could check the summary of such variable to make sure.

```{r}
summary(redDF$quality)
table(redDF$quality)
```

Unsuprisingly, `quality` does have classified values. **Based on this finding, it seems that linear regression is not suitable for this dataset. This is our initial hypothesis.**

### Correlation values {#corr}

The figure above shows that below relationships have a strong correlation. This also can be seen

 + Between `density` and `fixed.acidity` (0.67)
 + Between `fixed.acidity` and `citric.acid` (0.67)
 + Between `fixed.acidity` and `pH` (-0.68)
 + Between `free.sulfur.dioxide` and `total.sulfur.dioxide` (0.67)
     
Those perhaps indicate sufficiently high multicollinearity. We will highlight this issue and discuss it later in the [assumptions](#asum) section.

### P-values {#pval}

In addition, it is only `volatile.acidity` and `alcohol` which have the largest correlation value with `quality`. However, we also need to check the Pearson's correlation test based on the p-value. As seen in the figure above, the red stars in the upper triangle of the matrix indicate the significance. The more the stars exist, the more significant the relationship is. In order to be significant enough to the significance value (we use significance value (alpha) of 0.05), we need at least one star.

In this p-value analysis, we're only interested in considering the p-values of relationship between `quality` and each predictor variable. We can see that all variables have at least one star (meaning p-value less than pre-determined alpha (i.e. 0.05)), except `residual.sugar`. So, we won't consider such variable any longer.

## Modelling {#model}

### Splitting Train Datasets and Test Datasets

By using the dataset, we're going to split it up into 80% of data for train datasets and 20% of data for test datasets.

```{r}
set.seed(1)
sampleSize <- round(nrow(redDF)*0.8)
idx <- sample(seq_len(sampleSize), size = sampleSize)

X.train_red <- redDF[idx,]
X.test_red <- redDF[-idx,]

rownames(X.train_red) <- NULL
rownames(X.test_red) <- NULL
```

#### Create the Model

As mentioned in the [exploratory data analysis](#pval), we will employ all predictor variables, except `residual.sugar`, for the model. Let's create linear model from those variables.

```{r}
model_red1 <- lm(quality ~ fixed.acidity + volatile.acidity + citric.acid + chlorides + free.sulfur.dioxide + total.sulfur.dioxide + density + pH + sulphates + alcohol, 
                 data = X.train_red)
summary(model_red1)
```

From the `summary()` function above, it can be seen that approximately a half number of all predictor variables exhibits insignificance. Furthermore, the adjusted R-squared also performs poor result. Before we tackle with this issue, we should check the assumptions of the model.

#### Check the assumptions {#asum}

Since the linearity assumption has been discussed earlier in [this section](#corr), here, we're going to use three assumptions, i.e. normality, homoscedasticity, and multicollinearity.

 + Normality
     
By employing normality assumption, we'd like to have the residuals of the predicted value to approach the normal distribution. We can check this by plotting the residuals and using Shapiro-Wilk normality test. For the latter, we expect to have p-value more than significance value (i.e. 0.05) so that the null hypothesis is failed to reject.

In the [Preparation](#prep) chapter, we have declared a function to carry out this task called `CheckNormal()`. So, let's use it.
     
```{r}
CheckNormal(model = model_red1)
```

Although it seems the figure above indicates that the residuals tend to gather around 0 number (i.e. approaching to have normal distribution), we are unable to immediately believe this. We also have to check the results of Shapiro-Wilk normality test. And unfortunately, in the case of normality, our model shows poor results. The p-value is so small that H0 is rejected, meaning that the residuals is **not** distributed normally. We don't want this.

 + Homoscedasticity
 
In homoscedasticity aspect, we'd like to have residuals spreading constantly randomly, without generating any pattern. We have two approaches to examine this aspect, i.e. plotting the residuals vs the predicted values and performing the Breusch-Pagan test. As the function `CheckHomos()` to carry out this task has been declared already in [Preparation](#prep), we just need to use it.

```{r}
CheckHomos(model = model_red1)
```

As read above, the p-value is so small that null hypothesis is rejected. Moreover, the figure above also points out line-like patterns. This indeed states that the residuals generate patterns, meaning that heteroscedasticity exists. We don't want this.

 + Multicollinearity
 
In multicollinearity factor, inside the model, we'd like to have each predictor variable **not** demonstrating strong relationship with each other. We could examine this factor by inspecting their VIF (Variance Inflation Factor) score. We expect to have VIF score not greater than 10. We can perform this task by using the function `vif()` from the `car` package.

```{r warning=FALSE, message=FALSE}
library(car)
vif(model_red1)
```

By reading their score above, we see that the only largest value is `fixed.acidity`, i.e. ± 6.9. Fortunately, such score is still lower than 10. Therefore, in case of multicollinearity, our model performs satisfactorily.

## Model Improvements {#modimprov}

As stated in the previous chapter, by using `summary()` function and checking the assumptions, the model performs poor results, except in case of multicollinearity. Thus, any improvement has to be executed to decrease the its drawbacks.

### Check the Outliers

Firstly, let's check outliers of the dataset whether any high leverage high influence exist. We could use four plots here, i.e. Residuals vs Fitted, Normal Q-Q, Cook's Distance, and Residuals vs Leverage. For your information regarding those plots, you could read [here](https://data.library.virginia.edu/diagnostic-plots/).

```{r}
par(mfrow=c(2,2)) # Change the panel layout to 2 x 2 
lapply(c(1,2,4,5), # showing 4 types of plots
       function(x) plot(model_red1, 
                        which = x, 
                        # labels.id = 1:nrow(X.train_red),
                        cook.levels = c(0.05, 0.1))) %>% invisible()
```

From four figures above, we found there are some leverages with high influence, i.e. the observations with index 78, 202, 245, 274, and 1161. We're going to remove those rows.

```{r}
to.rm <- c(78,202,245,274,1161)
# X.train_red[to.rm,]
X.train_red <- X.train_red[-to.rm,]
rownames(X.train_red) <- NULL
```

After the outliers removed, a new model is generated, and also check its summary. 

```{r}
model_red2 <- lm(quality ~ fixed.acidity + volatile.acidity + citric.acid + chlorides + free.sulfur.dioxide + total.sulfur.dioxide + density + pH + sulphates + alcohol, 
                 data = X.train_red)
summary(model_red2)
```

It seems the new model performs more reliable. To make sure, let's check the adjusted R-squared values between two models.

```{r}
print("Adjusted R-squared for 1st model:")
ad.r.sq1 <- summary(model_red1)$adj.r.squared
ad.r.sq1
print("Adjusted R-squared for 2nd model:")
ad.r.sq2 <- summary(model_red2)$adj.r.squared
ad.r.sq2
print(paste0("The difference between both is ", round(ad.r.sq2-ad.r.sq1, 5)*100, "%"))
```

Well done. Adjusted R-squared increases by almost 2%. Now, we move on to try feature selection to improve the model.

### Feature Selection Implementation

We're going to employ step-wise algorithm for the feature selection method. We will use three directions of the algorithm, i.e. backward, forward, and both. First of all, we have to define the models for lower and upper threshold of the algorithm.

  + Create two models as threshold for the step wise algorithm
     
```{r}
model_redAlc <- lm(quality ~ alcohol, data = X.train_red)
# summary(model_redAlc)
model_redAll <- lm(quality ~ ., data = X.train_red)
# summary(model_redAll)
```

Now, let's carry out three approaches of step-wise algorithm.

  + Backward approach

```{r}
step(model_redAll, direction = "backward", trace = F)
```

```{r}
model.back_red <- lm(formula = quality ~ volatile.acidity + chlorides + free.sulfur.dioxide + 
    total.sulfur.dioxide + pH + sulphates + alcohol, data = X.train_red)
summary(model.back_red)
```

  + Forward approach

```{r}
step(model_redAlc, scope = list(lower = model_redAlc, upper = model_redAll),
     direction = "forward",
     trace = F)
```

```{r}
model.forw_red <- lm(formula = quality ~ alcohol + volatile.acidity + sulphates + total.sulfur.dioxide + chlorides + pH + free.sulfur.dioxide, 
    data = X.train_red)
summary(model.forw_red)
```

  + Both approach
  
```{r}
step(model_redAlc, scope = list(lower = model_redAlc, upper = model_redAll),
     direction = "both",
     trace = F)
```

```{r}
model.both_red <- lm(formula = quality ~ alcohol + volatile.acidity + sulphates + total.sulfur.dioxide + chlorides + pH + free.sulfur.dioxide, 
                     data = X.train_red)
summary(model.both_red)
```

All three approaches have been defined. Now, we're going to compare our all models so far by their adjusted R-squared.

```{r}
cat("Adjusted R-squared for 1st model:\n")
ad.r.sq1 <- summary(model_red1)$adj.r.squared
ad.r.sq1
cat("\nAdjusted R-squared for 2nd model:\n")
ad.r.sq2 <- summary(model_red2)$adj.r.squared
ad.r.sq2
cat("\nAdjusted R-squared for model using 'alcohol' variable only:\n")
ad.r.sqAlc <- summary(model_redAlc)$adj.r.squared
ad.r.sqAlc
cat("\nAdjusted R-squared for model using all variables:\n")
ad.r.sqAll <- summary(model_redAll)$adj.r.squared
ad.r.sqAll
cat("\nAdjusted R-squared for model with backward approach:\n")
ad.r.sqBack <- summary(model.back_red)$adj.r.squared
ad.r.sqBack
cat("\nAdjusted R-squared for model with forward approach:\n")
ad.r.sqForw <- summary(model.forw_red)$adj.r.squared
ad.r.sqForw
cat("\nAdjusted R-squared for model with both approach:\n")
ad.r.sqBoth <- summary(model.both_red)$adj.r.squared
ad.r.sqBoth
```

Evidently, after we have performed feature selection, we don't obtain the model with much higher performance. Instead, the best model so far is achieved not from such selection, but from manually including all available predictor variables. **Hence, from now on, the best model used will be the one with all predictor variables, i.e. `model_redAll`**

## Results and Discussions {#resdis}

In this chapter, we're going to discuss the best model so far and use it to predict the test dataset. Firstly, the performance of the model is discussed. Subsequently, the predictions will be carried out later.

### Check the Performances

Here, we're going to check the performances of the chosen model. The metrics used are Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and Mean Absolute Error (MAE). We just need the `indicator()` function defined in the beginning.

```{r}
indicator(model = model_redAll, y_pred = model_redAll$fitted.values, y_true = X.train_red$quality)
```

With performances as shown above, we will compare them to those of the prediction using test dataset.

### Predictions

Here, we're going to compare the performances of prediction using train dataset to those of using test dataset. The metrics used are MSE, RMSE, MAE, correlation between `y_pred` and `y_true`, and R-squared between `y_pred` and `y_true`. The plots between `y_pred` and `y_true` for each train dataset and test dataset also will be shown.

```{r}
cat("Performance using train dataset:\n")
metrics(y_pred = model_redAll$fitted.values, y_true = X.train_red$quality)

redPredict.back <- predict(model_redAll, newdata = X.test_red)
cat("\nPerformances using test dataset:\n")
metrics(y_pred = redPredict.back, y_true = X.test_red$quality)
```
```{r}
redFitted.back <- data.frame(qualityPred = model.back_red$fitted.values,
                                qualityAct = X.train_red$quality)
ggplot(redFitted.back, aes(x = qualityPred,
                       y = qualityAct)) +
     geom_point(aes(color = as.factor(qualityAct)), show.legend = F) +
     geom_smooth(method = "lm", se = F) +
     labs(title = "Predicted vs Actual Values Using Train Dataset",
          x = "Predicted quality",
          y = "Actual quality")
```
```{r}
redPredict.backDF <- data.frame(qualityPred = redPredict.back,
                                qualityAct = X.test_red$quality)
ggplot(redPredict.backDF, aes(x = qualityPred,
                       y = qualityAct)) +
     geom_point(aes(color = as.factor(qualityAct)), show.legend = F) +
     geom_smooth(method = "lm", se = F) +
     labs(title = "Predicted vs Actual Values Using Test Dataset",
          x = "Predicted quality",
          y = "Actual quality")
```

There are several points we can infer from the performance results above:

1. The model overfits the train dataset so that it performs poor when using test dataset.
2. As the model has defective results, it is unable to satisfactorily predict the target variable.
3. The plots verify poin number 2 that the model in fact is ineffective to predict the target variable. 

## Conclusions {#conc}

We have finished this article. Below are the points we can conclude from this article:

* A linear model has been created. The target variable is `quality`, whereas the attributes of physicochemical tests are as the predictor variables.

* The selected model is the one with all available variables. So, the model equation is as follows:

$\hat{Y} = \beta_0+\beta_1X_1+\beta_2X_2+\beta_3X_3+\beta_4X_4+\beta_5X_5+\beta_6X_6+\beta_7X_7+\beta_8X_8+\beta_9X_9+\beta_{10} X_{10}+\beta_{11}X_{11}$ 

where the following are values from the $\beta_0$ to $\beta_{11}$ and from $X_1$ to $X_{11}$:

```{r}
model_redAll$coefficients
```

* The statistical values (adjusted) R-squared, Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and Mean Absolute Error (MAE) of the selected model have been calculated.

```{r}
indicator(model = model_redAll, y_pred = model_redAll$fitted.values, y_true = X.train_red$quality)
```

* The selected model has been examined and improved using statistics tests (the assumptions and feature selection) and visualizations in [Modelling](#model) and [Model Improvements](#modimprov) chapters.

* The selected model has been interpretated in [Results and Discussions](#resdis) chapter.

* The selected model has been tested using test dataset test and discussed in [Results and Discussions](#resdis) chapter.

* The selected performs ineffective in modelling the train dataset. The best adjusted R-squared value produced is only at 0.3832.

* As the selected model show poor performances, it also demonstrates deficient results when predicting the test dataset.

* As stated earlier in [histogram](#hist) and [scatter plots](#scat) sections that it is found an initial hypothesis that the target variable has classified values instead of continuous values so it seems the linear regression is not suitable with the dataset, such hypothesis has been proven. All results and discussions in this article verify it.

* As mentioned above, therefore, it can be concluded that for the type of this dataset, **in particular a target variable with classified values, it is not recommended to model the data using linear regression.**

* For future study, other algorithms will be applied to model this wine quality dataset.